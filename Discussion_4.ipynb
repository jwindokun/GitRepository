{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems, Security, Privacy, Ethics and Government Regulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For this discussion, the article I selected is Cohen et al., (2014) article titled “The Legal And Ethical Concerns That Arise From Using Complex Predictive Analytics In Health Care”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommender Systems, Predictive Analysis and other such systems are all a subset of Machine Learning, which is because computers can find patterns in large datasets, which are not easily understandable by humans and therefore create algorithms to make sense of these data in order to make recommendations or predictions on newer items.\n",
    "Machine Learning can be supervised, where by human input is necessary to tell the computer what steps to take in coming up with its algorithm – an example would be Linear Regression, semi-supervised whereby there is some input by humans (k-nearest neighbor for example or SVD, where k is specified) or unsupervised, neural networks for example (although there is work going on to understand and refine what happens in a neural network).\n",
    "The end point of Machine Learning is to allow the system make recommendations or predictions on what the items/outcomes are most probable and to give a score (prediction) on how likely this is to happen as compared to other outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the Article:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohen et al., (2014) in their article discuss how predictive systems have become more and more important in healthcare, and the problems that arise with the use of these systems. Most predictive systems in healthcare at present are more tailored towards population outcomes and not necessarily the best outcome for the individual patient, an example that they give is a recommender system advising the physician to withhold an expensive treatment/intervention for a patient because the perceived benefit to that patient is thought to be low as compared to another patient. In this situation, the recommender does not recommend what is best for the patient, who after all might be inclined to want the intervention even though the probability of success is low as compared to the 100% probability of a bad outcome if the treatment if withheld.\n",
    "This issue is similar to what Amazon (and other vendors do) for example does when it only recommends items that it wishes to sell (business rules).\n",
    "Cohen discussed the multitude of problems that are associated with this from lack of consent from the patient in being involved in what is basically an experiment, lack of transparency due to the fact that these algorithms are secret and no one knows what they actually are programmed to do, and possible legal ramifications, if physicians do follow these recommendations or do not (and patient harm ensures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohen et al’s article focuses on the issues with predictive analysis in healthcare however; this issue is prevalent in the entire Machine Learning, Recommender Systems and Predictive Analytics Universe. \n",
    "As these systems become more and more prevalent, we rely on them more and more and give up our normal responses, which can be fatal (http://www.theverge.com/2016/6/30/12072408/tesla-autopilot-car-crash-death-autonomous-model-s).   \n",
    "These recommender systems have been put into use without a thorough discussion of the how these systems work and any limitations of such systems. For example the Compas Assessment (http://doc.wi.gov/about/doc-overview/office-of-the-secretary/office-of-reentry/compas-assessment-tool) is a predictive analysis tool that is used for:\n",
    "\n",
    "      Automated risk and needs assessment and unified case planning system. This actuarial risk assessment system contains  offender information specifically designed to determine their risk and needs and inform dynamic case plans that will guide the offender throughout his or her lifecycle in the criminal justice system\n",
    "      \n",
    "This tool is proprietary and therefore the inner working algorithms are not known however, it does use gender and other factors in its recommendation (apparently it or similar tools use race too) and its recommendations can make a huge difference in sentencing – for example 1 year in jail versus 6 years.\n",
    "Propublica.org has an excellent article that discusses this titled “Machine Bias”-  Kirchner et al., (2016).\n",
    "Due to issues such as this, Amarasingham et al 2016 has recommended that such algorithms be made public so that they can be analyzed.\n",
    "Machine Learning Systems are amoral and then do not make any distinction between what is good or bad, this becomes an issue when organizations use them and program them for business requirements. Farivar (2016) and Pearson (2014) have described how predictive analytics and recommender systems are being used to target people based on race and income with worse economic products (high interest credit cards etc). This is similar to seating fast food restaurants and payday loan businesses in economically disadvantaged neighborhoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning, Predictive Analysis and Recommender Systems are here to stay. These systems by themselves are amoral, and therefore they do have to be programmed by humans to take into consideration human morals of right and wrong. It is imperative that a national consensus be obtained on what these algorithms are allowed to do, what type of oversight has to be maintained on them, the ethics of using them, the transparency that has to be put in place with the use of these algorithms and more importantly that those that these algorithms are used on understand this is happening, understand that these recommendations might not be in their best interest but based on some business rules that may be detrimental to them, and that they have the right to question the recommendations of these algorithms and to opt out if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cohen, I. G., Amarasingham, R., Shah, A., Xie, B., & Lo, B. (2014). The Legal And Ethical Concerns That Arise From Using Complex Predictive Analytics In Health Care. Health Affairs, 33(7), 1139–1147. http://doi.org/10.1377/hlthaff.2014.0048\n",
    "Amarasingham, R., Audet, A.-M., Bates, D., Cohen, I. G., Entwistle, M., Escobar, G., … Xie, B. (2016). Consensus Statement on Electronic Health Predictive Analytics: A Guiding Framework \n",
    "\n",
    "to Address Challenges. eGEMs (Generating Evidence & Methods to Improve Patient Outcomes), 4(1). http://doi.org/10.13063/2327-9214.1163\n",
    "Farivar, C. (2016, June 29). To study possibly racist algorithms, professors have to sue the US. Retrieved July 4, 2016, from http://arstechnica.com/tech-policy/2016/06/do-housing-jobs-sites-have-racist-algorithms-academics-sue-to-find-out/\n",
    "\n",
    "\n",
    "Pearson, J. (2014, September 16). How Big Data Could Discriminate. Retrieved July 4, 2016, from http://motherboard.vice.com/read/why-the-federal-trade-commission-thinks-big-data-could-be-discriminatory\n",
    "Kirchner, J. A., Surya Mattu, Jeff Larson, Lauren. (2016, May 23). Machine Bias: There’s Software Used Across the Country to Predict Future Criminals. And it’s Biased Against Blacks. Retrieved July 4, 2016, from https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
